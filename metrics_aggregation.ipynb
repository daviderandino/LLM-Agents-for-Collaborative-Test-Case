{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f87aa40",
   "metadata": {},
   "source": [
    "# Metrics Aggregation by Experiment\n",
    "This notebook aggregates metrics from all_metrics.csv by experiment name,\n",
    "showing mean and standard deviation for coverage, mutation score, and tokens used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb59711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"src\")\n",
    "\n",
    "from utils.plot_metrics import plot_metrics\n",
    "\n",
    "# Execute plot_metrics to show individual run metrics\n",
    "plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334f4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61bb72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metrics data\n",
    "df = pd.read_csv(\"all_metrics.csv\")\n",
    "print(f\"Loaded {len(df)} rows from all_metrics.csv\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdb01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics by experiment_name\n",
    "# Focus on: coverage_percent, mutation_score_percent, total_tokens\n",
    "\n",
    "aggregated = (\n",
    "    df.groupby(\"experiment_name\")\n",
    "    .agg(\n",
    "        {\n",
    "            \"coverage_percent\": [\"mean\", \"std\"],\n",
    "            \"mutation_score_percent\": [\"mean\", \"std\"],\n",
    "            \"total_tokens\": [\"mean\", \"std\"],\n",
    "        }\n",
    "    )\n",
    "    .round(2)\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "aggregated.columns = [\"_\".join(col).strip() for col in aggregated.columns.values]\n",
    "aggregated = aggregated.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86eca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table for reference\n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"Experiment\"] = aggregated[\"experiment_name\"]\n",
    "result_df[\"Coverage Mean\"] = aggregated[\"coverage_percent_mean\"]\n",
    "result_df[\"Coverage Std\"] = aggregated[\"coverage_percent_std\"]\n",
    "result_df[\"Mutation Mean\"] = aggregated[\"mutation_score_percent_mean\"]\n",
    "result_df[\"Mutation Std\"] = aggregated[\"mutation_score_percent_std\"]\n",
    "result_df[\"Tokens Mean\"] = aggregated[\"total_tokens_mean\"].astype(int)\n",
    "result_df[\"Tokens Std\"] = aggregated[\"total_tokens_std\"].round(0).astype(int)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0210f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Single grouped chart for all metrics\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "experiments = aggregated[\"experiment_name\"]\n",
    "x = np.arange(len(experiments))\n",
    "width = 0.28\n",
    "\n",
    "# Normalize tokens to percentage scale to fit with coverage/mutation\n",
    "tokens_norm = (\n",
    "    aggregated[\"total_tokens_mean\"] / aggregated[\"total_tokens_mean\"].max()\n",
    ") * 100\n",
    "\n",
    "bars1 = ax.bar(\n",
    "    x - width,\n",
    "    aggregated[\"coverage_percent_mean\"],\n",
    "    width,\n",
    "    label=\"Coverage (%)\",\n",
    "    color=\"steelblue\",\n",
    ")\n",
    "bars2 = ax.bar(\n",
    "    x,\n",
    "    aggregated[\"mutation_score_percent_mean\"],\n",
    "    width,\n",
    "    label=\"Mutation Score (%)\",\n",
    "    color=\"coral\",\n",
    ")\n",
    "bars3 = ax.bar(\n",
    "    x + width, tokens_norm, width, label=\"Tokens (normalized %)\", color=\"seagreen\"\n",
    ")\n",
    "\n",
    "ax.set_title(\"Metrics per Experiment (single chart)\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(experiments, rotation=45, ha=\"right\")\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872e8d0",
   "metadata": {},
   "source": [
    "# Graphs for the report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dc312e",
   "metadata": {},
   "source": [
    "## Comparing the single agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1f656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"all_metrics.csv\")\n",
    "\n",
    "# Filter Single Agents\n",
    "df_single = df[df[\"experiment_name\"].str.startswith(\"single_\")].copy()\n",
    "\n",
    "# Calculate Metrics\n",
    "df_single[\"total_tests\"] = df_single[\"n_passed_tests\"] + df_single[\"n_failed_tests\"]\n",
    "df_single[\"pass_rate\"] = df_single.apply(\n",
    "    lambda row: (\n",
    "        row[\"n_passed_tests\"] / row[\"total_tests\"] * 100\n",
    "        if row[\"total_tests\"] > 0\n",
    "        else 0\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Aggregate\n",
    "metrics = [\"pass_rate\", \"coverage_percent\", \"mutation_score_percent\"]\n",
    "agg_df = df_single.groupby(\"experiment_name\")[metrics].mean().reset_index()\n",
    "\n",
    "# Clean Names\n",
    "agg_df[\"experiment_name\"] = agg_df[\"experiment_name\"].str.replace(\"single_\", \"\")\n",
    "\n",
    "# Normalize data for Parallel Coordinates (0-1 Scale)\n",
    "scaler = MinMaxScaler()\n",
    "df_norm = agg_df.copy()\n",
    "df_norm[metrics] = scaler.fit_transform(agg_df[metrics])\n",
    "\n",
    "# Rename columns for the plot\n",
    "df_norm.columns = [\"Agent\", \"Pass Rate\", \"Coverage\", \"Mutation Score\"]\n",
    "\n",
    "# Melt for plotting\n",
    "df_melted = pd.melt(\n",
    "    df_norm, id_vars=\"Agent\", var_name=\"Metric\", value_name=\"NormalizedScore\"\n",
    ")\n",
    "\n",
    "# Plotting Parallel Coordinates Style using Lineplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Draw the lines\n",
    "sns.lineplot(\n",
    "    data=df_melted,\n",
    "    x=\"Metric\",\n",
    "    y=\"NormalizedScore\",\n",
    "    hue=\"Agent\",\n",
    "    palette=\"viridis\",\n",
    "    linewidth=3,\n",
    "    marker=\"o\",\n",
    "    markersize=8,\n",
    ")\n",
    "\n",
    "# Formatting\n",
    "plt.title(\"Qualitative Comparison (Single Agents)\", fontsize=16)\n",
    "plt.ylabel(\"Relative Score (0=Worst, 1=Best)\", fontsize=12)\n",
    "plt.xlabel(\"\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(True, axis=\"x\", linestyle=\"--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs(\"report\", exist_ok=True)\n",
    "# plt.savefig(\"report/parallel_coordinates_quality_only.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfef359",
   "metadata": {},
   "source": [
    "## Baseline vs collaborative vs competitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb1ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"all_metrics.csv\")\n",
    "\n",
    "# Consistent color palette across charts\n",
    "ARCH_COLORS = {\n",
    "    \"Single Agent\": \"#1f77b4\",\n",
    "    \"Collaborative\": \"#2ca02c\",\n",
    "    \"Competitive\": \"#ff7f0e\",\n",
    "}\n",
    "BASELINE_COLORS = {\n",
    "    \"Best Single\": \"#1f77b4\",\n",
    "    \"Avg Single\": \"#aec7e8\",\n",
    "}\n",
    "\n",
    "\n",
    "# 1. Categorize experiments\n",
    "def get_category(name):\n",
    "    if name.startswith(\"single_\"):\n",
    "        return \"Single Agent\"\n",
    "    elif name.startswith(\"collaborative_\"):\n",
    "        return \"Collaborative\"\n",
    "    elif name.startswith(\"competitive_\"):\n",
    "        return \"Competitive\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "df[\"category\"] = df[\"experiment_name\"].apply(get_category)\n",
    "\n",
    "# 2. Calculate Metrics\n",
    "df[\"total_tests\"] = df[\"n_passed_tests\"] + df[\"n_failed_tests\"]\n",
    "df[\"pass_rate\"] = df.apply(\n",
    "    lambda row: (\n",
    "        row[\"n_passed_tests\"] / row[\"total_tests\"] * 100\n",
    "        if row[\"total_tests\"] > 0\n",
    "        else 0\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# 3. Prepare Data for Multi-Agent (Collaborative & Competitive)\n",
    "multi_agent_df = df[df[\"category\"].isin([\"Collaborative\", \"Competitive\"])]\n",
    "\n",
    "# 4. Prepare Data for Baselines\n",
    "# Baseline 1: Best Single Agent (gptoss120B)\n",
    "best_baseline_df = df[df[\"experiment_name\"] == \"single_gptoss120B\"]\n",
    "best_baseline_means = best_baseline_df[\n",
    "    [\"pass_rate\", \"coverage_percent\", \"mutation_score_percent\"]\n",
    "].mean()\n",
    "\n",
    "# Baseline 2: Average of All Single Agents\n",
    "avg_baseline_df = df[df[\"category\"] == \"Single Agent\"]\n",
    "avg_baseline_means = avg_baseline_df[\n",
    "    [\"pass_rate\", \"coverage_percent\", \"mutation_score_percent\"]\n",
    "].mean()\n",
    "\n",
    "# 5. Plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle(\"Architecture Comparison: Multi-Agent vs Baselines\", fontsize=16)\n",
    "\n",
    "metrics = [\n",
    "    (\"pass_rate\", \"Pass Rate (%)\", 80, 105),\n",
    "    (\"coverage_percent\", \"Coverage (%)\", 80, 105),\n",
    "    (\"mutation_score_percent\", \"Mutation Score (%)\", 50, 85),\n",
    "]\n",
    "\n",
    "categories = [\"Collaborative\", \"Competitive\"]\n",
    "colors = [ARCH_COLORS[\"Collaborative\"], ARCH_COLORS[\"Competitive\"]]\n",
    "\n",
    "for i, (metric, title, ylim_min, ylim_max) in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Baseline Values\n",
    "    best_val = best_baseline_means[metric]\n",
    "    avg_val = avg_baseline_means[metric]\n",
    "\n",
    "    # Plot Bars with seaborn for confidence intervals\n",
    "    sns.barplot(\n",
    "        data=multi_agent_df,\n",
    "        x=\"category\",\n",
    "        y=metric,\n",
    "        hue=\"category\",\n",
    "        order=categories,\n",
    "        hue_order=categories,\n",
    "        palette=colors,\n",
    "        ax=ax,\n",
    "        errorbar=(\"ci\", 95),\n",
    "        capsize=0.1,\n",
    "        alpha=0.8,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    # Plot Baseline Lines\n",
    "    # Line 1: Best Baseline\n",
    "    ax.axhline(\n",
    "        y=best_val,\n",
    "        color=BASELINE_COLORS[\"Best Single\"],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f\"Best Single Agent: {best_val:.1f}%\",\n",
    "    )\n",
    "\n",
    "    # Line 2: Average Baseline\n",
    "    ax.axhline(\n",
    "        y=avg_val,\n",
    "        color=BASELINE_COLORS[\"Avg Single\"],\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "        label=f\"Avg Single Agent: {avg_val:.1f}%\",\n",
    "    )\n",
    "\n",
    "    # Styling\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylim(ylim_min, ylim_max)\n",
    "    ax.set_ylabel(\"Score (%)\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc=\"lower center\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"report/multi_agent_vs_baselines_bars.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2e440",
   "metadata": {},
   "source": [
    "## Big vs mixed vs small (vs collaborative vs competitive vs single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da05cee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"all_metrics.csv\")\n",
    "\n",
    "# Consistent color palette across charts\n",
    "ARCH_COLORS = {\n",
    "    \"Single Agent\": \"#1f77b4\",\n",
    "    \"Collaborative\": \"#2ca02c\",\n",
    "    \"Competitive\": \"#ff7f0e\",\n",
    "}\n",
    "\n",
    "# --- 1. Data Preparation & Classification ---\n",
    "\n",
    "# Define Model Sizes\n",
    "big_models = [\"gptoss120B\", \"llama70B\", \"llama70b\"]\n",
    "small_models = [\n",
    "    \"llamascout17B\",\n",
    "    \"llama8B\",\n",
    "    \"gptoss20B\",\n",
    "    \"llamascout17b\",\n",
    "    \"llama8b\",\n",
    "    \"gptoss20b\",\n",
    "]\n",
    "\n",
    "\n",
    "def classify_experiment(exp_name):\n",
    "    parts = exp_name.split(\"_\")\n",
    "    arch_type = parts[0]\n",
    "    models = parts[1:]\n",
    "\n",
    "    # 1. Architecture\n",
    "    if arch_type == \"single\":\n",
    "        return \"Single Agent\", None\n",
    "    elif arch_type == \"collaborative\":\n",
    "        architecture = \"Collaborative\"\n",
    "    elif arch_type == \"competitive\":\n",
    "        architecture = \"Competitive\"\n",
    "    else:\n",
    "        return \"Other\", None\n",
    "\n",
    "    # 2. Composition\n",
    "    has_big = False\n",
    "    has_small = False\n",
    "\n",
    "    for model in models:\n",
    "        m_lower = model.lower()\n",
    "        is_big = any(b.lower() in m_lower for b in big_models)\n",
    "        is_small = any(s.lower() in m_lower for s in small_models)\n",
    "\n",
    "        if is_big:\n",
    "            has_big = True\n",
    "        if is_small:\n",
    "            has_small = True\n",
    "\n",
    "    if has_big and not has_small:\n",
    "        composition = \"Big Only\"\n",
    "    elif has_small and not has_big:\n",
    "        composition = \"Small Only\"\n",
    "    elif has_big and has_small:\n",
    "        composition = \"Mixed (Big + Small)\"\n",
    "    else:\n",
    "        composition = \"Unknown\"\n",
    "\n",
    "    return architecture, composition\n",
    "\n",
    "\n",
    "# Apply classification\n",
    "df[\"Architecture\"], df[\"Composition\"] = zip(\n",
    "    *df[\"experiment_name\"].apply(classify_experiment)\n",
    ")\n",
    "\n",
    "# Metrics Calculation\n",
    "df[\"total_tests\"] = df[\"n_passed_tests\"] + df[\"n_failed_tests\"]\n",
    "df[\"pass_rate\"] = df.apply(\n",
    "    lambda row: (\n",
    "        row[\"n_passed_tests\"] / row[\"total_tests\"] * 100\n",
    "        if row[\"total_tests\"] > 0\n",
    "        else 0\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Rename for plotting\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"pass_rate\": \"Pass Rate\",\n",
    "        \"coverage_percent\": \"Coverage\",\n",
    "        \"mutation_score_percent\": \"Mutation Score\",\n",
    "    }\n",
    ")\n",
    "\n",
    "metrics_list = [\"Pass Rate\", \"Coverage\", \"Mutation Score\"]\n",
    "\n",
    "# --- 2. Calculate Baselines ---\n",
    "\n",
    "# Best Single Agent (gptoss120B)\n",
    "best_single_df = df[df[\"experiment_name\"] == \"single_gptoss120B\"]\n",
    "best_baselines = best_single_df[metrics_list].mean()\n",
    "\n",
    "# Average Single Agent\n",
    "avg_single_df = df[df[\"Architecture\"] == \"Single Agent\"]\n",
    "avg_baselines = avg_single_df[metrics_list].mean()\n",
    "\n",
    "# --- 3. Filter Data for Plotting ---\n",
    "# Keep only Multi-Agent rows with valid composition\n",
    "plot_df = df[\n",
    "    (df[\"Architecture\"].isin([\"Collaborative\", \"Competitive\"]))\n",
    "    & (df[\"Composition\"] != \"Unknown\")\n",
    "]\n",
    "\n",
    "# --- 4. Visualization ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "fig.suptitle(\n",
    "    \"Impact of Architecture & Model Size: Collaborative vs Competitive\", fontsize=18\n",
    ")\n",
    "\n",
    "# Order for X-axis\n",
    "comp_order = [\"Big Only\", \"Mixed (Big + Small)\", \"Small Only\"]\n",
    "# Colors for Architecture\n",
    "arch_palette = {\n",
    "    \"Collaborative\": ARCH_COLORS[\"Collaborative\"],\n",
    "    \"Competitive\": ARCH_COLORS[\"Competitive\"],\n",
    "}\n",
    "\n",
    "for i, metric in enumerate(metrics_list):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Plot Bar Chart\n",
    "    sns.barplot(\n",
    "        data=plot_df,\n",
    "        x=\"Composition\",\n",
    "        y=metric,\n",
    "        hue=\"Architecture\",\n",
    "        order=comp_order,\n",
    "        hue_order=[\"Collaborative\", \"Competitive\"],\n",
    "        palette=arch_palette,\n",
    "        ax=ax,\n",
    "        errorbar=(\"ci\", 95),  # Confidence Interval\n",
    "        capsize=0.1,\n",
    "    )\n",
    "\n",
    "    # Add Baselines\n",
    "    # Best Single\n",
    "    ax.axhline(\n",
    "        y=best_baselines[metric],\n",
    "        color=ARCH_COLORS[\"Single Agent\"],\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f\"Best Single Agent\",\n",
    "    )\n",
    "    # Avg Single\n",
    "    ax.axhline(\n",
    "        y=avg_baselines[metric],\n",
    "        color=\"#aec7e8\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=2,\n",
    "        label=f\"Avg Single Agent\",\n",
    "    )\n",
    "\n",
    "    # Styling\n",
    "    ax.set_title(metric, fontsize=14)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Score (%)\" if i == 0 else \"\")\n",
    "\n",
    "    # Y-Limits adjustment to make room for text/visuals\n",
    "    if metric == \"Pass Rate\" or metric == \"Coverage\":\n",
    "        ax.set_ylim(80, 105)\n",
    "    else:  # Mutation Score\n",
    "        ax.set_ylim(40, 85)\n",
    "\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "    # Legend handling: Only show in the last plot or bottom\n",
    "    if i == 1:  # Center legend for shared info? Or just put it in the last one.\n",
    "        # Let's put legend in the middle plot, below\n",
    "        pass\n",
    "\n",
    "    # Remove individual legends to create a unified one later or keep per plot if clean\n",
    "    ax.legend().remove()\n",
    "\n",
    "# Unified Legend\n",
    "# Get handles/labels from one of the axes\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "# Add baseline handles manually if not in barplot handles (axhline doesn't automatically add to seaborn legend usually, need to check)\n",
    "# Actually, axhline labels are in ax.get_legend_handles_labels() if labeled.\n",
    "h, l = axes[0].get_legend_handles_labels()\n",
    "# h contains bars + lines.\n",
    "# h[0:2] are bars, h[2:4] are lines usually.\n",
    "\n",
    "fig.legend(\n",
    "    h,\n",
    "    l,\n",
    "    loc=\"lower center\",\n",
    "    bbox_to_anchor=(0.5, -0.05),\n",
    "    ncol=4,\n",
    "    fontsize=12,\n",
    "    frameon=False,\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])  # Make space for legend\n",
    "# plt.savefig(\"report/architecture_size_comparison_split.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e01a13",
   "metadata": {},
   "source": [
    "## Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c185c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load Data\n",
    "df = pd.read_csv(\"all_metrics.csv\")\n",
    "\n",
    "# Consistent color palette across charts\n",
    "ARCH_COLORS = {\n",
    "    \"Single Agent\": \"#1f77b4\",\n",
    "    \"Collaborative\": \"#2ca02c\",\n",
    "    \"Competitive\": \"#ff7f0e\",\n",
    "}\n",
    "EFFICIENCY_COLORS = {\n",
    "    \"Average\": \"#95a5a6\",\n",
    "    \"Best\": ARCH_COLORS[\"Single Agent\"],\n",
    "}\n",
    "\n",
    "# 2. Metric Calculation\n",
    "# Pass Rate\n",
    "df[\"total_tests\"] = df[\"n_passed_tests\"] + df[\"n_failed_tests\"]\n",
    "df[\"pass_rate\"] = df.apply(\n",
    "    lambda row: (\n",
    "        row[\"n_passed_tests\"] / row[\"total_tests\"] if row[\"total_tests\"] > 0 else 0\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# File-Normalized Tokens (Relative Cost)\n",
    "file_mean_tokens = df.groupby(\"file\")[\"total_tokens\"].transform(\"mean\")\n",
    "df[\"tokens_relative_cost\"] = df[\"total_tokens\"] / file_mean_tokens\n",
    "\n",
    "# Relative Efficiency Score = (Quality / Cost)\n",
    "# Quality = PassRate * Coverage * Mutation\n",
    "# Cost = Normalized Tokens\n",
    "df[\"efficiency\"] = (\n",
    "    df[\"pass_rate\"]\n",
    "    * df[\"coverage_percent\"]\n",
    "    * df[\"mutation_score_percent\"]\n",
    "    / df[\"tokens_relative_cost\"]\n",
    ")\n",
    "\n",
    "# 3. Data Preparation for \"Average\" vs \"Best\" Comparison\n",
    "categories = [\"Single Agent\", \"Collaborative\", \"Competitive\"]\n",
    "plot_data = []\n",
    "\n",
    "\n",
    "def get_category(name):\n",
    "    if name.startswith(\"single_\"):\n",
    "        return \"Single Agent\"\n",
    "    if name.startswith(\"collaborative_\"):\n",
    "        return \"Collaborative\"\n",
    "    if name.startswith(\"competitive_\"):\n",
    "        return \"Competitive\"\n",
    "    return \"Other\"\n",
    "\n",
    "\n",
    "df[\"category\"] = df[\"experiment_name\"].apply(get_category)\n",
    "\n",
    "combined_rows = []\n",
    "for cat in categories:\n",
    "    cat_df = df[df[\"category\"] == cat]\n",
    "\n",
    "    # \"Average\" Group: All runs in this category\n",
    "    avg_rows = cat_df.copy()\n",
    "    avg_rows[\"Condition\"] = \"Average\"\n",
    "    avg_rows[\"Architecture\"] = cat\n",
    "    combined_rows.append(avg_rows[[\"Architecture\", \"Condition\", \"efficiency\"]])\n",
    "\n",
    "    # \"Best\" Group: Runs from the single best experiment (highest mean efficiency)\n",
    "    exp_means = cat_df.groupby(\"experiment_name\")[\"efficiency\"].mean()\n",
    "    best_exp_name = exp_means.idxmax()\n",
    "    best_rows = cat_df[cat_df[\"experiment_name\"] == best_exp_name].copy()\n",
    "    best_rows[\"Condition\"] = \"Best\"\n",
    "    best_rows[\"Architecture\"] = cat\n",
    "    combined_rows.append(best_rows[[\"Architecture\", \"Condition\", \"efficiency\"]])\n",
    "\n",
    "final_plot_df = pd.concat(combined_rows)\n",
    "\n",
    "# 4. Plotting\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.set_style(\"whitegrid\")\n",
    "palette = {\"Average\": EFFICIENCY_COLORS[\"Average\"], \"Best\": EFFICIENCY_COLORS[\"Best\"]}\n",
    "\n",
    "ax = sns.barplot(\n",
    "    data=final_plot_df,\n",
    "    x=\"Architecture\",\n",
    "    y=\"efficiency\",\n",
    "    hue=\"Condition\",\n",
    "    palette=palette,\n",
    "    errorbar=(\"ci\", 95),  # Confidence Interval\n",
    "    capsize=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "plt.title(\n",
    "    \"Efficiency Analysis: Average vs Best Configuration per Architecture\", fontsize=16\n",
    ")\n",
    "plt.ylabel(\"Relative Efficiency Score\\n(Quality / Relative Cost)\", fontsize=12)\n",
    "plt.xlabel(\"Architecture\", fontsize=12)\n",
    "plt.legend(title=\"Configuration\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"report/efficiency_final.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4se_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
