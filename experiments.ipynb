{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491ee08f",
   "metadata": {},
   "source": [
    "# Agents For Test Case Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83342c79",
   "metadata": {},
   "source": [
    "## Single Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb75c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.experiment_runner --config configs/experiments/gpt_based/single_gpt4_baseline.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a65acc7",
   "metadata": {},
   "source": [
    "## Multi Agent Collaborative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.experiment_runner --config configs/experiments/gpt_based/collaborative_strong_generator.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.experiment_runner --config configs/experiments/gpt_based/collaborative_strong_planner.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69223fe4",
   "metadata": {},
   "source": [
    "## Multi Agent Competitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.experiment_runner --config configs/experiments/gpt_based/competitive_smart.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64569f",
   "metadata": {},
   "source": [
    "## GPT-based Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8932b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m src.experiment_runner --config configs/experiments/gpt_based/collaborative_strong_generator.yaml\n",
    "!python -m src.experiment_runner --config configs/experiments/gpt_based/collaborative_strong_planner.yaml\n",
    "!python -m src.experiment_runner --config configs/experiments/gpt_based/collaborative_strong.yaml\n",
    "!python -m src.experiment_runner --config configs/experiments/gpt_based/collaborative_weak.yaml\n",
    "!python -m src.experiment_runner --config configs/experiments/gpt_based/competitive_dumb_planner_smart_workers.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f489a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Loading Experiment: configs/experiments/gpt_based/collaborative_weak.yaml\n",
      "2026-01-13 19:34:43,026 - Orchestrator - INFO - üöÄ Starting Experiment. Strategy: collaborative_agents\n",
      "2026-01-13 19:34:43,026 - Orchestrator - INFO - üìÇ Input Path: data/input_code\n",
      "2026-01-13 19:34:43,026 - Orchestrator - INFO - üå°Ô∏è  Temperature: 0.2\n",
      "2026-01-13 19:34:43,026 - Orchestrator - INFO - üìù Found 5 file(s) to process.\n",
      "2026-01-13 19:34:43,026 - Orchestrator - INFO - --- Processing: d10_hotel_reservation.py ---\n",
      "\u001b[36m\n",
      "--- STEP 1.1: PLANNING FROM SCRATCH ---\u001b[0m\n",
      "2026-01-13 19:34:53,758 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m--- STEP 2.1: GENERATING TESTS FROM SCRATCH---\u001b[0m\n",
      "2026-01-13 19:34:53,866 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2026-01-13 19:34:53,866 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 40.000000 seconds\n",
      "2026-01-13 19:35:41,473 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m--- STEP 3: EXECUTING PYTEST ---\u001b[0m\n",
      "\u001b[32m--- EXECUTION RESULT: Coverage=81% 9 Passed 15 Failed ---\u001b[0m\n",
      "\u001b[33m--- STEP 2.2: FIXING FAILED TESTS ---\u001b[0m\n",
      "2026-01-13 19:35:42,245 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2026-01-13 19:35:42,246 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 44.000000 seconds\n",
      "2026-01-13 19:36:31,549 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m--- STEP 3: EXECUTING PYTEST ---\u001b[0m\n",
      "\u001b[32m--- EXECUTION RESULT: Coverage=100% 23 Passed 1 Failed ---\u001b[0m\n",
      "\u001b[33m--- STEP 2.2: FIXING FAILED TESTS ---\u001b[0m\n",
      "2026-01-13 19:36:32,271 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2026-01-13 19:36:32,271 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 35.000000 seconds\n",
      "2026-01-13 19:37:13,028 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m--- STEP 3: EXECUTING PYTEST ---\u001b[0m\n",
      "\u001b[32m--- EXECUTION RESULT: Coverage=100% 24 Passed 0 Failed ---\u001b[0m\n",
      "2026-01-13 19:37:52,287 - Orchestrator - INFO - --- Processing: d07_library.py ---\n",
      "\u001b[36m\n",
      "--- STEP 1.1: PLANNING FROM SCRATCH ---\u001b[0m\n",
      "2026-01-13 19:37:55,122 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m--- STEP 2.1: GENERATING TESTS FROM SCRATCH---\u001b[0m\n",
      "2026-01-13 19:37:59,373 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m--- STEP 3: EXECUTING PYTEST ---\u001b[0m\n",
      "\u001b[32m--- EXECUTION RESULT: Coverage=96% 8 Passed 7 Failed ---\u001b[0m\n",
      "\u001b[33m--- STEP 2.2: FIXING FAILED TESTS ---\u001b[0m\n",
      "2026-01-13 19:38:00,199 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2026-01-13 19:38:00,199 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 17.000000 seconds\n",
      "2026-01-13 19:38:26,420 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m--- STEP 3: EXECUTING PYTEST ---\u001b[0m\n",
      "\u001b[32m--- EXECUTION RESULT: Coverage=96% 13 Passed 2 Failed ---\u001b[0m\n",
      "\u001b[33m--- STEP 2.2: FIXING FAILED TESTS ---\u001b[0m\n",
      "2026-01-13 19:38:27,136 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2026-01-13 19:38:27,136 - groq._base_client - INFO - Retrying request to /openai/v1/chat/completions in 16.000000 seconds\n",
      "2026-01-13 19:38:55,330 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "\u001b[36m--- STEP 3: EXECUTING PYTEST ---\u001b[0m\n",
      "\u001b[32m--- EXECUTION RESULT: Coverage=96% 13 Passed 2 Failed ---\u001b[0m\n",
      "\u001b[33m--- STEP 2.2: FIXING FAILED TESTS ---\u001b[0m\n",
      "2026-01-13 19:38:56,043 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2026-01-13 19:38:56,043 - Orchestrator - ERROR - Failed to process d07_library.py: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kbtbshk8emva5fqc55j7vfxx` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199999, Requested 2094. Please try again in 15m4.176s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/experiment_runner.py\", line 66, in run_experiment\n",
      "    file_result = run_collaborative_agents(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/multi_agent_collaborative_runner.py\", line 32, in run_collaborative_agents\n",
      "    final_state = agents.invoke()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/MultiAgentCollaborativeGraph.py\", line 522, in invoke\n",
      "    final_state = self.graph.invoke(self.initial_state)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 3068, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2643, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/MultiAgentCollaborativeGraph.py\", line 426, in _generation_node\n",
      "    response = chain.invoke(invoke_args)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3143, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 398, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 927, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1221, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_groq/chat_models.py\", line 590, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/resources/chat/completions.py\", line 461, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kbtbshk8emva5fqc55j7vfxx` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199999, Requested 2094. Please try again in 15m4.176s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "During task with name 'developer' and id '5922b297-5505-616b-0664-a4ac5f36fe06'\n",
      "2026-01-13 19:38:56,052 - Orchestrator - INFO - --- Processing: d03_stack.py ---\n",
      "\u001b[36m\n",
      "--- STEP 1.1: PLANNING FROM SCRATCH ---\u001b[0m\n",
      "2026-01-13 19:38:56,243 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2026-01-13 19:38:56,243 - Orchestrator - ERROR - Failed to process d03_stack.py: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kbtbshk8emva5fqc55j7vfxx` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199998, Requested 746. Please try again in 5m21.408s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/experiment_runner.py\", line 66, in run_experiment\n",
      "    file_result = run_collaborative_agents(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/multi_agent_collaborative_runner.py\", line 32, in run_collaborative_agents\n",
      "    final_state = agents.invoke()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/MultiAgentCollaborativeGraph.py\", line 522, in invoke\n",
      "    final_state = self.graph.invoke(self.initial_state)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 3068, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2643, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/MultiAgentCollaborativeGraph.py\", line 163, in _plan_node\n",
      "    response = chain.invoke(invoke_args)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3143, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 398, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 927, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1221, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_groq/chat_models.py\", line 590, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/resources/chat/completions.py\", line 461, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kbtbshk8emva5fqc55j7vfxx` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199998, Requested 746. Please try again in 5m21.408s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "During task with name 'planner' and id '22f660ae-7df0-84a9-d4fb-7c62109ae4cb'\n",
      "2026-01-13 19:38:56,245 - Orchestrator - INFO - --- Processing: d01_bank_account.py ---\n",
      "\u001b[36m\n",
      "--- STEP 1.1: PLANNING FROM SCRATCH ---\u001b[0m\n",
      "2026-01-13 19:38:56,432 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2026-01-13 19:38:56,433 - Orchestrator - ERROR - Failed to process d01_bank_account.py: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kbtbshk8emva5fqc55j7vfxx` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199998, Requested 608. Please try again in 4m21.792s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/experiment_runner.py\", line 66, in run_experiment\n",
      "    file_result = run_collaborative_agents(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/multi_agent_collaborative_runner.py\", line 32, in run_collaborative_agents\n",
      "    final_state = agents.invoke()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/MultiAgentCollaborativeGraph.py\", line 522, in invoke\n",
      "    final_state = self.graph.invoke(self.initial_state)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 3068, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2643, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/MultiAgentCollaborativeGraph.py\", line 163, in _plan_node\n",
      "    response = chain.invoke(invoke_args)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3143, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 398, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 927, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1221, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_groq/chat_models.py\", line 590, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/resources/chat/completions.py\", line 461, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kbtbshk8emva5fqc55j7vfxx` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199998, Requested 608. Please try again in 4m21.792s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "During task with name 'planner' and id '661911b5-3c7f-fe35-ae0c-a480b742fc5e'\n",
      "2026-01-13 19:38:56,434 - Orchestrator - INFO - --- Processing: d04_linked_list.py ---\n",
      "\u001b[36m\n",
      "--- STEP 1.1: PLANNING FROM SCRATCH ---\u001b[0m\n",
      "2026-01-13 19:38:56,633 - httpx - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "2026-01-13 19:38:56,634 - Orchestrator - ERROR - Failed to process d04_linked_list.py: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kbtbshk8emva5fqc55j7vfxx` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199997, Requested 809. Please try again in 5m48.191999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/experiment_runner.py\", line 66, in run_experiment\n",
      "    file_result = run_collaborative_agents(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/multi_agent_collaborative_runner.py\", line 32, in run_collaborative_agents\n",
      "    final_state = agents.invoke()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/MultiAgentCollaborativeGraph.py\", line 522, in invoke\n",
      "    final_state = self.graph.invoke(self.initial_state)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 3068, in invoke\n",
      "    for chunk in self.stream(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/main.py\", line 2643, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/_runner.py\", line 167, in tick\n",
      "    run_with_retry(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/pregel/_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 656, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langgraph/_internal/_runnable.py\", line 400, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/luigi/Documents/MSc-PoliTO/II_anno/LLM4SE/LLM-Agents-for-Collaborative-Test-Case/src/agents/multi_agent_collaborative/MultiAgentCollaborativeGraph.py\", line 163, in _plan_node\n",
      "    response = chain.invoke(invoke_args)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3143, in invoke\n",
      "    input_ = context.run(step.invoke, input_, config)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 398, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1117, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 927, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1221, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/langchain_groq/chat_models.py\", line 590, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/resources/chat/completions.py\", line 461, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/_base_client.py\", line 1242, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/llm4se_project/lib/python3.11/site-packages/groq/_base_client.py\", line 1044, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "groq.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for model `openai/gpt-oss-20b` in organization `org_01kbtbshk8emva5fqc55j7vfxx` service tier `on_demand` on tokens per day (TPD): Limit 200000, Used 199997, Requested 809. Please try again in 5m48.191999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}\n",
      "During task with name 'planner' and id 'f6cfb536-4069-bdc4-e4eb-a24dbcc261ba'\n",
      "2026-01-13 19:38:56,636 - Orchestrator - INFO - --- üèÅ Experiment Complete ---\n",
      "üìù Appended metrics to results/master_log.csv\n",
      "üìä Results saved to results/collaborative_weak/metrics.json\n"
     ]
    }
   ],
   "source": [
    "!python -m src.experiment_runner --config configs/experiments/gpt_based/collaborative_weak.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm4se_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
